{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks notebook source\n",
    "\n",
    " # Simplified Mining Operations Data Generation\n",
    " \n",
    " This notebook generates focused synthetic data for mining operations efficiency analysis.\n",
    " \n",
    " ## Core Data Entities:\n",
    " - **Truck Productivity Events**: Operational events with delays and breakdowns (80% productive, 20% non-productive)\n",
    " - **Truck Cycles**: Loading and dumping transactions with timing data (no source/destination references)\n",
    " - **Time Usage Model**: Event categorization for operational efficiency analysis\n",
    " \n",
    " ## Geographic Context:\n",
    " - **Location**: Pilbara region of Western Australia\n",
    " - **Coordinates**: Latitude -21.5° to -23.5°S, Longitude 115.0° to 125.0°E\n",
    " - **Coverage**: Major mining areas including Karratha, Port Hedland, Newman, and surrounding regions\n",
    " \n",
    " ## Data Volume:\n",
    " - 50 trucks\n",
    " - 30 days of operational data\n",
    " - 30-60 events per truck per day (67,500 total events estimated)\n",
    " - Realistic breakdown/delay patterns (3% breakdown, 12% delay, 85% productive)\n",
    " - Time usage model with operational efficiency categorization\n",
    " \n",
    " ## Realistic Data Features:\n",
    " - **Payload Utilization**: 80-110% of truck capacity (realistic loading scenarios)\n",
    " - **Truck Capacities**: 280-500 tonnes (realistic mining truck range)\n",
    " - **Geographic Accuracy**: Pilbara region coordinates for authentic mining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, when, isnan, isnull\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from builtins import round as py_round, min as py_min, max as py_max\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Spark session is already available in Databricks\n",
    "\n",
    "# Set the Unity Catalog context\n",
    "spark.sql(\"USE CATALOG mining_operations\")\n",
    "spark.sql(\"USE SCHEMA production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "NUM_TRUCKS = 50\n",
    "DAYS_TO_GENERATE = 30\n",
    "\n",
    "# Variable activity parameters for realistic daily variation\n",
    "MIN_EVENTS_PER_DAY = 800   # Low activity days\n",
    "MAX_EVENTS_PER_DAY = 2000  # High activity days\n",
    "BASE_EVENTS_PER_DAY = 1400 # Average baseline\n",
    "\n",
    "# Productivity parameters (adjusted for more realistic distribution)\n",
    "PRODUCTIVE_EVENT_PROBABILITY = 0.85  # 85% of events are productive\n",
    "BREAKDOWN_PROBABILITY = 0.03         # 3% breakdowns (1-5% range)\n",
    "DELAY_PROBABILITY = 0.12             # 12% delays (10-15% range)\n",
    "\n",
    "# Pilbara region coordinate bounds (Western Australia)\n",
    "PILBARA_LAT_MIN = -23.5   # Southern boundary\n",
    "PILBARA_LAT_MAX = -21.5   # Northern boundary\n",
    "PILBARA_LON_MIN = 115.0   # Western boundary\n",
    "PILBARA_LON_MAX = 125.0   # Eastern boundary\n",
    "\n",
    "# Payload utilization parameters\n",
    "MIN_PAYLOAD_UTILIZATION = 0.80  # 80% of truck capacity\n",
    "MAX_PAYLOAD_UTILIZATION = 1.10  # 110% of truck capacity (overloaded)\n",
    "\n",
    "# Date range\n",
    "START_DATE = datetime.now() - timedelta(days=DAYS_TO_GENERATE)\n",
    "END_DATE = datetime.now()\n",
    "\n",
    "print(f\"Generating {DAYS_TO_GENERATE} days of simplified mining operations data\")\n",
    "print(f\"Date range: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Variable events per day: {MIN_EVENTS_PER_DAY}-{MAX_EVENTS_PER_DAY}\")\n",
    "print(f\"Productivity breakdown: {PRODUCTIVE_EVENT_PROBABILITY*100:.0f}% productive, {BREAKDOWN_PROBABILITY*100:.0f}% breakdowns, {DELAY_PROBABILITY*100:.0f}% delays\")\n",
    "print(f\"Geographic region: Pilbara, Western Australia\")\n",
    "print(f\"Coordinate bounds: Lat {PILBARA_LAT_MIN}° to {PILBARA_LAT_MAX}°S, Lon {PILBARA_LON_MIN}° to {PILBARA_LON_MAX}°E\")\n",
    "print(f\"Payload utilization: {MIN_PAYLOAD_UTILIZATION*100:.0f}% to {MAX_PAYLOAD_UTILIZATION*100:.0f}% of truck capacity\")\n",
    "# Calculate estimated total events (30-60 events per truck per day)\n",
    "avg_events_per_truck_per_day = 45  # Average of 30-60\n",
    "estimated_total_events = DAYS_TO_GENERATE * NUM_TRUCKS * avg_events_per_truck_per_day\n",
    "print(f\"Estimated total events: {estimated_total_events:,} (45 events per truck per day average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simplified truck data\n",
    "def generate_trucks():\n",
    "    truck_data = []\n",
    "    \n",
    "    truck_types = [\"CAT 797F\", \"Komatsu 930E\", \"Liebherr T 284\", \"Belaz 75710\", \"Hitachi EH5000\"]\n",
    "    \n",
    "    for i in range(NUM_TRUCKS):\n",
    "        truck_type = random.choice(truck_types)\n",
    "        \n",
    "        truck_data.append({\n",
    "            \"truck_id\": f\"T{i+1:03d}\",\n",
    "            \"truck_type\": truck_type,\n",
    "            \"capacity_tonnes\": random.randint(280, 500),\n",
    "            \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(truck_data)\n",
    "\n",
    "trucks_df = generate_trucks()\n",
    "print(f\"✅ Generated {trucks_df.count()} trucks\")\n",
    "trucks_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate truck cycles (simplified transactions)\n",
    "def generate_truck_cycles():\n",
    "    cycles = []\n",
    "    \n",
    "    # Create truck capacity mapping for realistic payload generation\n",
    "    truck_capacities = {}\n",
    "    for i in range(NUM_TRUCKS):\n",
    "        truck_id = f\"T{i+1:03d}\"\n",
    "        # Use same capacity generation logic as trucks\n",
    "        truck_capacities[truck_id] = random.randint(280, 500)\n",
    "    \n",
    "    # Generate timestamps for the entire period\n",
    "    current_time = START_DATE\n",
    "    \n",
    "    while current_time < END_DATE:\n",
    "        # Determine daily activity volume\n",
    "        day_of_week = current_time.weekday()\n",
    "        \n",
    "        # Weekend effect (reduced activity)\n",
    "        if day_of_week >= 5:  # Saturday, Sunday\n",
    "            daily_multiplier = random.uniform(0.6, 0.8)\n",
    "        else:\n",
    "            daily_multiplier = random.uniform(0.8, 1.2)\n",
    "        \n",
    "        # Calculate daily cycle count\n",
    "        base_daily_cycles = int(BASE_EVENTS_PER_DAY * daily_multiplier)\n",
    "        daily_cycles = py_max(MIN_EVENTS_PER_DAY, \n",
    "                             py_min(MAX_EVENTS_PER_DAY, base_daily_cycles))\n",
    "        \n",
    "        print(f\"Day {current_time.strftime('%Y-%m-%d')}: {daily_cycles} cycles\")\n",
    "        \n",
    "        # Generate cycles for the day\n",
    "        for cycle_num in range(daily_cycles):\n",
    "            # Distribute cycles throughout the day\n",
    "            minute_in_day = random.randint(0, 1439)\n",
    "            timestamp = current_time + timedelta(minutes=minute_in_day)\n",
    "            \n",
    "            # Randomly select truck\n",
    "            truck_id = f\"T{random.randint(1, NUM_TRUCKS):03d}\"\n",
    "            \n",
    "            # Generate cycle timing\n",
    "            loading_duration = random.randint(5, 15)\n",
    "            travel_duration = random.randint(15, 45)\n",
    "            dumping_duration = random.randint(3, 10)\n",
    "            total_cycle_duration = loading_duration + travel_duration + dumping_duration\n",
    "            \n",
    "            # Calculate timestamps\n",
    "            loading_start = timestamp\n",
    "            loading_end = loading_start + timedelta(minutes=loading_duration)\n",
    "            travel_start = loading_end\n",
    "            travel_end = travel_start + timedelta(minutes=travel_duration)\n",
    "            dumping_start = travel_end\n",
    "            dumping_end = dumping_start + timedelta(minutes=dumping_duration)\n",
    "            \n",
    "            # Generate payload based on truck capacity (80-110% of capacity)\n",
    "            truck_capacity = truck_capacities[truck_id]\n",
    "            payload_utilization = random.uniform(MIN_PAYLOAD_UTILIZATION, MAX_PAYLOAD_UTILIZATION)\n",
    "            payload_tonnes = int(truck_capacity * payload_utilization)\n",
    "            \n",
    "            cycles.append({\n",
    "                \"cycle_id\": f\"CYC{timestamp.strftime('%Y%m%d%H%M')}{random.randint(1000, 9999)}\",\n",
    "                \"truck_id\": truck_id,\n",
    "                \"payload_tonnes\": payload_tonnes,\n",
    "                \"loading_start_time\": loading_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"loading_end_time\": loading_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"travel_start_time\": travel_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"travel_end_time\": travel_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dumping_start_time\": dumping_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dumping_end_time\": dumping_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"total_cycle_duration_minutes\": total_cycle_duration,\n",
    "                \"cycle_status\": \"Completed\",\n",
    "                \"created_at\": timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "        \n",
    "        current_time += timedelta(days=1)\n",
    "    \n",
    "    return spark.createDataFrame(cycles)\n",
    "\n",
    "print(\"�� Generating truck cycles data...\")\n",
    "truck_cycles_df = generate_truck_cycles()\n",
    "print(f\"✅ Generated {truck_cycles_df.count():,} truck cycles\")\n",
    "truck_cycles_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate truck productivity events (80% productive with occasional issues)\n",
    "def generate_truck_productivity_events():\n",
    "    events = []\n",
    "    \n",
    "    # Event types with realistic probabilities (80% productive)\n",
    "    productive_events = [\n",
    "        \"Loading\", \"Traveling\", \"Dumping\", \"Available\", \"En Route\"\n",
    "    ]\n",
    "    \n",
    "    breakdown_events = [\n",
    "        \"Engine Failure\", \"Hydraulic Issue\", \"Tire Puncture\", \"Electrical Problem\", \n",
    "        \"Transmission Issue\", \"Brake Malfunction\", \"Fuel System Problem\"\n",
    "    ]\n",
    "    \n",
    "    delay_events = [\n",
    "        \"Weather Delay\", \"Traffic Delay\", \"Loading Delay\", \"Dumping Delay\", \n",
    "        \"Maintenance Delay\", \"Driver Break\", \"Safety Check\"\n",
    "    ]\n",
    "    \n",
    "    # Generate events for each truck throughout the day\n",
    "    current_time = START_DATE\n",
    "    \n",
    "    while current_time < END_DATE:\n",
    "        for truck_id in range(1, NUM_TRUCKS + 1):\n",
    "            # Generate 30-60 events per truck per day\n",
    "            num_events = random.randint(30, 60)\n",
    "            \n",
    "            for event in range(num_events):\n",
    "                # Random time during the day\n",
    "                event_time = current_time + timedelta(\n",
    "                    hours=random.randint(0, 23),\n",
    "                    minutes=random.randint(0, 59)\n",
    "                )\n",
    "                \n",
    "                # Determine event type based on probabilities (85% productive, 3% breakdown, 12% delay)\n",
    "                rand = random.random()\n",
    "                if rand < PRODUCTIVE_EVENT_PROBABILITY:  # 85% productive events\n",
    "                    # Productive event\n",
    "                    event_type = random.choice(productive_events)\n",
    "                    duration_minutes = random.randint(5, 60)\n",
    "                    event_category = \"Productive\"\n",
    "                elif rand < PRODUCTIVE_EVENT_PROBABILITY + BREAKDOWN_PROBABILITY:  # 3% breakdown events\n",
    "                    # Breakdown event\n",
    "                    event_type = random.choice(breakdown_events)\n",
    "                    duration_minutes = random.randint(30, 240)\n",
    "                    event_category = \"Breakdown\"\n",
    "                else:  # 12% delay events\n",
    "                    # Delay event\n",
    "                    event_type = random.choice(delay_events)\n",
    "                    duration_minutes = random.randint(15, 120)\n",
    "                    event_category = \"Delay\"\n",
    "                \n",
    "                events.append({\n",
    "                    \"event_id\": f\"EVT{event_time.strftime('%Y%m%d%H%M')}{random.randint(1000, 9999)}\",\n",
    "                    \"truck_id\": f\"T{truck_id:03d}\",\n",
    "                    \"event_timestamp\": event_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"event_type\": event_type,\n",
    "                    \"event_category\": event_category,\n",
    "                    \"duration_minutes\": duration_minutes,\n",
    "                    \"event_details\": f\"{event_type} - {event_category}\",\n",
    "                    # Pilbara region coordinates (Western Australia)\n",
    "                    # Latitude: -21.5 to -23.5 (South), Longitude: 115.0 to 125.0 (East)\n",
    "                    \"location_x\": py_round(random.uniform(PILBARA_LON_MIN, PILBARA_LON_MAX), 6),  # Longitude (East)\n",
    "                    \"location_y\": py_round(random.uniform(PILBARA_LAT_MIN, PILBARA_LAT_MAX), 6),  # Latitude (South)\n",
    "                    \"created_at\": event_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "        \n",
    "        current_time += timedelta(days=1)\n",
    "    \n",
    "    return spark.createDataFrame(events)\n",
    "\n",
    "print(\"🔄 Generating truck productivity events (85% productive, 3% breakdown, 12% delay)...\")\n",
    "truck_productivity_events_df = generate_truck_productivity_events()\n",
    "print(f\"✅ Generated {truck_productivity_events_df.count():,} productivity events\")\n",
    "truck_productivity_events_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simplified time usage model\n",
    "def generate_time_usage_model():\n",
    "    time_usage_data = []\n",
    "    \n",
    "    # Simplified time usage categories matching our event types\n",
    "    time_usage_categories = [\n",
    "        # Productive Events\n",
    "        (\"Loading\", \"Productive\", \"Operations\", \"Loading materials into truck\", 90.0, True),\n",
    "        (\"Traveling\", \"Productive\", \"Operations\", \"Moving between locations\", 85.0, True),\n",
    "        (\"Dumping\", \"Productive\", \"Operations\", \"Unloading materials at destination\", 90.0, True),\n",
    "        (\"Available\", \"Productive\", \"Standby\", \"Truck available for assignment\", 100.0, True),\n",
    "        (\"En Route\", \"Productive\", \"Operations\", \"Traveling to destination\", 85.0, True),\n",
    "        \n",
    "        # Breakdown Events\n",
    "        (\"Engine Failure\", \"Breakdown\", \"Mechanical\", \"Engine mechanical failure\", 0.0, False),\n",
    "        (\"Hydraulic Issue\", \"Breakdown\", \"Mechanical\", \"Hydraulic system problem\", 0.0, False),\n",
    "        (\"Tire Puncture\", \"Breakdown\", \"Mechanical\", \"Tire damage requiring repair\", 0.0, False),\n",
    "        (\"Electrical Problem\", \"Breakdown\", \"Mechanical\", \"Electrical system malfunction\", 0.0, False),\n",
    "        (\"Transmission Issue\", \"Breakdown\", \"Mechanical\", \"Transmission system failure\", 0.0, False),\n",
    "        (\"Brake Malfunction\", \"Breakdown\", \"Mechanical\", \"Brake system problem\", 0.0, False),\n",
    "        (\"Fuel System Problem\", \"Breakdown\", \"Mechanical\", \"Fuel system malfunction\", 0.0, False),\n",
    "        \n",
    "        # Delay Events\n",
    "        (\"Weather Delay\", \"Delay\", \"External\", \"Weather-related operational delay\", 0.0, False),\n",
    "        (\"Traffic Delay\", \"Delay\", \"External\", \"Traffic congestion delay\", 0.0, False),\n",
    "        (\"Loading Delay\", \"Delay\", \"Operational\", \"Loading process delay\", 0.0, False),\n",
    "        (\"Dumping Delay\", \"Delay\", \"Operational\", \"Dumping process delay\", 0.0, False),\n",
    "        (\"Maintenance Delay\", \"Delay\", \"Planned\", \"Scheduled maintenance delay\", 0.0, False),\n",
    "        (\"Driver Break\", \"Delay\", \"Personal\", \"Driver rest and meal break\", 0.0, False),\n",
    "        (\"Safety Check\", \"Delay\", \"Safety\", \"Safety inspection and check\", 0.0, False)\n",
    "    ]\n",
    "    \n",
    "    for event_type, category, subcategory, description, target_efficiency, is_productive in time_usage_categories:\n",
    "        time_usage_data.append({\n",
    "            \"event_type\": event_type,\n",
    "            \"category\": category,\n",
    "            \"subcategory\": subcategory,\n",
    "            \"description\": description,\n",
    "            \"target_efficiency_percentage\": target_efficiency,\n",
    "            \"is_productive\": is_productive,\n",
    "            \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(time_usage_data)\n",
    "\n",
    "print(\"🔄 Generating simplified time usage model...\")\n",
    "time_usage_model_df = generate_time_usage_model()\n",
    "print(f\"✅ Generated {time_usage_model_df.count()} time usage model records\")\n",
    "time_usage_model_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save simplified data to Unity Catalog volume as parquet files\n",
    "print(\"💾 Saving simplified data to Unity Catalog volume...\")\n",
    "\n",
    "# Configuration for volume path\n",
    "CATALOG_NAME = \"mining_operations\"\n",
    "SCHEMA_NAME = \"production\"\n",
    "VOLUME_NAME = \"raw_data\"\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}\"\n",
    "\n",
    "# Save trucks data\n",
    "trucks_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_PATH}/trucks\")\n",
    "print(f\"✅ Saved trucks data: {trucks_df.count()} records\")\n",
    "\n",
    "# Save truck cycles data\n",
    "truck_cycles_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_PATH}/truck_cycles\")\n",
    "print(f\"✅ Saved truck cycles data: {truck_cycles_df.count():,} records\")\n",
    "\n",
    "# Save truck productivity events data\n",
    "truck_productivity_events_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_PATH}/truck_productivity_events\")\n",
    "print(f\"✅ Saved truck productivity events data: {truck_productivity_events_df.count():,} records\")\n",
    "\n",
    "# Save time usage model data\n",
    "time_usage_model_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_PATH}/time_usage_model\")\n",
    "print(f\"✅ Saved time usage model data: {time_usage_model_df.count()} records\")\n",
    "\n",
    "print(\"\\n🎉 All simplified data successfully saved to Unity Catalog volume!\")\n",
    "print(f\"Volume location: {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show simplified data summary\n",
    "print(\"\\n📊 Simplified Data Summary:\")\n",
    "print(f\"Trucks: {trucks_df.count()}\")\n",
    "print(f\"Truck Cycles: {truck_cycles_df.count():,}\")\n",
    "print(f\"Productivity Events: {truck_productivity_events_df.count():,}\")\n",
    "print(f\"Time Usage Model: {time_usage_model_df.count()}\")\n",
    "print(f\"Total records: {trucks_df.count() + truck_cycles_df.count() + truck_productivity_events_df.count() + time_usage_model_df.count():,}\")\n",
    "\n",
    "# Show productivity breakdown\n",
    "print(f\"\\n📈 Productivity Breakdown:\")\n",
    "print(f\"Productive events: ~{PRODUCTIVE_EVENT_PROBABILITY*100:.0f}%\")\n",
    "print(f\"Breakdown events: ~{BREAKDOWN_PROBABILITY*100:.0f}%\")\n",
    "print(f\"Delay events: ~{DELAY_PROBABILITY*100:.0f}%\")\n",
    "print(f\"Events per truck per day: 30-60 (average: 45)\")\n",
    "\n",
    "# Show geographic information\n",
    "print(f\"\\n🗺️ Geographic Context:\")\n",
    "print(f\"Region: Pilbara, Western Australia\")\n",
    "print(f\"Coordinate bounds: Latitude {PILBARA_LAT_MIN}° to {PILBARA_LAT_MAX}°S\")\n",
    "print(f\"Longitude bounds: {PILBARA_LON_MIN}° to {PILBARA_LON_MAX}°E\")\n",
    "print(f\"Coverage: Major mining areas including Karratha, Port Hedland, Newman\")\n",
    "\n",
    "# Show payload utilization information\n",
    "print(f\"\\n🚛 Payload Utilization:\")\n",
    "print(f\"Range: {MIN_PAYLOAD_UTILIZATION*100:.0f}% to {MAX_PAYLOAD_UTILIZATION*100:.0f}% of truck capacity\")\n",
    "print(f\"Truck capacity range: 280-500 tonnes\")\n",
    "print(f\"Expected payload range: {int(280*MIN_PAYLOAD_UTILIZATION)}-{int(500*MAX_PAYLOAD_UTILIZATION)} tonnes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "python3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
